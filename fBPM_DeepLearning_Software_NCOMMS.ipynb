{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fBPM Deep Learning analysis\n",
    "Script that analyzes multiple datasets with deep learning after eachother, and saves the analysis of a .txt file in a folder with the same name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from random import sample\n",
    "from numpy.random import shuffle\n",
    "import os, shutil\n",
    "#base_dir = '/Users/s136769/stack/MBx Python/MSD script'\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import sklearn as sk\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from lmfit import Model, Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filename):\n",
    "    nparray=np.array(pd.read_csv(filename, header=None))\n",
    "    return nparray\n",
    "\n",
    "def obtain_xy_per_particle(data_txt):\n",
    "    num_part = data_txt.shape[0]\n",
    "    x_coord = np.zeros((num_part,int(data_txt.shape[1]/2-1)))\n",
    "    y_coord = np.zeros((num_part,int(data_txt.shape[1]/2-1)))\n",
    "    for i in range(num_part):\n",
    "        x0 = data_txt[i,0]\n",
    "        y0 = data_txt[i,1]\n",
    "        x_coord[i,:] = data_txt[i,2::2]+x0\n",
    "        y_coord[i,:] = data_txt[i,3::2]+y0\n",
    "    \n",
    "    return [x_coord, y_coord]\n",
    "\n",
    "def X_test_from_timeseries(x,y,N_timesteps):\n",
    "    X_train=np.zeros((x.shape[0]-N_timesteps+1, N_timesteps, 2))\n",
    "    for i in range(x.shape[0]-N_timesteps+1):\n",
    "        X_train[i,:,0]=x[i:i+N_timesteps]\n",
    "        X_train[i,:,1]=y[i:i+N_timesteps]\n",
    "        \n",
    "    return X_train\n",
    "\n",
    "def obtain_state_per_pos(state_per_avg, measurementwindow):\n",
    "    y=np.zeros(state_per_avg.shape[0]+measurementwindow-1)\n",
    "    for i in range(y.shape[0]):\n",
    "        if i<measurementwindow-1:\n",
    "            y[i]=np.mean(state_per_avg[:i+1])\n",
    "        elif i>state_per_avg.shape[0]-1:\n",
    "            y[i]=np.mean(state_per_avg[i-measurementwindow+1:])\n",
    "        else:\n",
    "            y[i]=np.mean(state_per_avg[i-measurementwindow+1:i+1])\n",
    "    y=np.round(y)\n",
    "    return y\n",
    "\n",
    "def smoother_unev(a, smooth): #Watch out!! works only OK when inputting a uneven smooth prm -> otherwise avg doesnt work\n",
    "    ret=np.zeros(a.shape[0])\n",
    "    smooth_prm=(smooth-1)/2\n",
    "    for i in range(a.shape[0]):\n",
    "        if i+1<=smooth_prm:\n",
    "            ret[i]=np.mean(a[:i+1+int(smooth_prm)])\n",
    "        elif i+1>a.shape[0]-smooth_prm:\n",
    "            ret[i]=np.mean(a[i-int(smooth_prm):a.shape[0]])\n",
    "        else:\n",
    "            ret[i]=np.mean(a[i-int(smooth_prm):i+1+int(smooth_prm)])\n",
    "    ret=np.round(ret)\n",
    "    return ret\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def diff_coef_over_time(x,y,measurementwindow,no_MSD_val, FrameRate):\n",
    "    DtraceWindow=np.zeros((no_MSD_val,x.shape[0]-measurementwindow+1))\n",
    "    weight = np.zeros(no_MSD_val)\n",
    "    for dt in range(no_MSD_val):\n",
    "        SDtrace = np.square(x[dt+1:]-x[:-dt-1]) + np.square(y[dt+1:]-y[:-dt-1])\n",
    "        SDtraceWindow = moving_average(SDtrace, measurementwindow-dt-1)\n",
    "        \n",
    "        DtraceWindow[dt,:]=SDtraceWindow*FrameRate/(4*(dt+1)) #(no_MSD, no_x_val)\n",
    "        Vrel = (dt+1)*(2*np.square(dt+1)+1)/(measurementwindow-dt)\n",
    "        weight[dt]=1/Vrel\n",
    "        \n",
    "    weight_norm = weight/np.sum(weight)\n",
    "    D_tot=np.sum(np.multiply(DtraceWindow.T, weight_norm).T, axis=0)\n",
    "    return D_tot\n",
    "\n",
    "def expand_D_val(D, measurementwindow):\n",
    "    D_new=np.zeros(D.shape[0]+measurementwindow-1)\n",
    "    for i in range(D_new.shape[0]):\n",
    "        if i<measurementwindow-1:\n",
    "            D_new[i]=np.mean(D[:i+1])\n",
    "        elif i>D.shape[0]-1:\n",
    "            D_new[i]=np.mean(D[i-measurementwindow+1:])\n",
    "        else:\n",
    "            D_new[i]=D[i-measurementwindow+1]\n",
    "    return D_new\n",
    "\n",
    "def obtain_pred_state2(x_test, y_test, state2_test, measurementwindow, norm, model, smooth):\n",
    "    i=0\n",
    "    save=0\n",
    "    pred_state2=np.zeros(state2_test.shape[0])\n",
    "    while i<state2_test.shape[0]-1:\n",
    "        if state2_test[i]>0:\n",
    "            begin_point=i\n",
    "            \n",
    "        while state2_test[i]>0 and i<state2_test.shape[0]-1:\n",
    "            i=i+1\n",
    "            save=1\n",
    "        \n",
    "        if save==1:\n",
    "            if state2_test[begin_point:i].shape[0]<=measurementwindow:\n",
    "                pred_state2[begin_point:i]=1\n",
    "                save=0\n",
    "            else:            \n",
    "                x_arr=x_test[begin_point:i]\n",
    "                y_arr=y_test[begin_point:i]\n",
    "\n",
    "                X_test=X_test_from_timeseries(x_arr,y_arr,measurementwindow)\n",
    "                # Normalize in the same way as the training data\n",
    "                for k in range(X_test.shape[0]):\n",
    "                    X_test[k,:,:]-=X_test[k,0,:] # minus the first coordinate to start the trajectories in the same spot.\n",
    "            \n",
    "                X_test=X_test/norm #normalize by the bound radius of the first state.\n",
    "                \n",
    "                Y_pred=model.predict(X_test) #You could round these predictions -> or set different probability threshold.\n",
    "                pred_state2[begin_point:i]=smoother_unev(obtain_state_per_pos(Y_pred, measurementwindow)+1, smooth)\n",
    "                save=0    \n",
    "        i=i+1\n",
    "    pred_state2[-1]=pred_state2[-2]\n",
    "    return pred_state2\n",
    "\n",
    "# april 2021 -> addition of option to downscale 60 fps to 30 fps.\n",
    "def downscale_FR60_to_FR30(x_per_part, y_per_part):\n",
    "    x_per_part30 = x_per_part[:,np.arange(0,x_per_part.shape[1]-1,2)]\n",
    "    y_per_part30 = y_per_part[:,np.arange(0,y_per_part.shape[1]-1,2)]\n",
    "    N_frames = x_per_part30.shape[1]\n",
    "    FrameRate=30\n",
    "    print('DOWNSCALED TO 30 FPS: The number of frames per particle is equal to %1.0f' %N_frames + ', which corresponds to %1.0f seconds.' %np.floor(N_frames/FrameRate))\n",
    "    return [x_per_part30, y_per_part30, N_frames, FrameRate]\n",
    "\n",
    "# Reduce the traces to 1 frame per second -> used in the find_mislocalizations function to speed up the search.\n",
    "def reduce_to_1frame_ps(x, FrameRate):\n",
    "    N_frames = x.shape[1]\n",
    "    x_1FR = x[:,np.linspace(0,N_frames-FrameRate, int(N_frames/FrameRate)).astype(int)]\n",
    "    return x_1FR\n",
    "\n",
    "# Filter out the particles that show mislocalizations -> which means that two particles move in the vicinity of each other \n",
    "# -> consequently, the localization algorithm cannot distinguish the particles and the \n",
    "def find_mislocalizations(x_per_part, y_per_part, FrameRate):\n",
    "    # Round up to a micrometer -> afterwards, check whether particles have same coordinate value in rounded micrometers.\n",
    "    x_per_part_round = np.round(x_per_part)\n",
    "    y_per_part_round = np.round(y_per_part)\n",
    "    # Look at the coordinate every second, rather than every frame.\n",
    "    x_round_short = reduce_to_1frame_ps(x_per_part_round, FrameRate)\n",
    "    y_round_short = reduce_to_1frame_ps(y_per_part_round, FrameRate)\n",
    "    \n",
    "    # set the threshold for number of coordinates two traces must have in common on same timestep to be flagged as mislocalized.\n",
    "    # Currently this is 10 -> hence the 1/10.\n",
    "    N_overlap_threshold = np.round(x_round_short.shape[1]/10)\n",
    "    Mislocalization = np.array([])\n",
    "    for n_part in range(x_round_short.shape[0]):\n",
    "        #Record wether different particles have the same trace (same coordinates at same time)\n",
    "        x_same_coord = x_round_short==x_round_short[n_part,:]\n",
    "        y_same_coord = y_round_short==y_round_short[n_part,:]\n",
    "        xy_overlap = np.logical_and(x_same_coord, y_same_coord)\n",
    "        N_overlap_per_particle = np.sum(xy_overlap,axis=1)\n",
    "        index_misl = np.where(N_overlap_per_particle>N_overlap_threshold)[0]\n",
    "        # Need to filter out the particle n_part itself.\n",
    "        if index_misl.shape[0]>1:\n",
    "            index_misl = np.delete(index_misl,np.where(index_misl==n_part)[0])\n",
    "            Mislocalization = np.append(Mislocalization, index_misl)\n",
    "    #sort and remove duplicates from the mislocalization array    \n",
    "    Mislocalization_final = np.unique(Mislocalization)\n",
    "    return Mislocalization_final\n",
    "\n",
    "def MSD_from_xy(x,y,windowsize,no_MSD_val):\n",
    "    DtraceWindow = np.zeros((no_MSD_val, x.size-windowsize+1))\n",
    "    for dt in range(no_MSD_val):\n",
    "        SDtrace=np.square(x[1+dt:]-x[:-dt-1])+np.square(y[1+dt:]-y[:-dt-1]) #squared displacement     \n",
    "        SDtraceWindow=moving_average(SDtrace,windowsize-dt-1)\n",
    "        DtraceWindow[dt,:]=SDtraceWindow #;%*FrameRate/(4*dt); %was SDtraceWindow/(4*dt)\n",
    "    return DtraceWindow\n",
    "\n",
    "def find_freely_diffusing_particles(act01, x_per_part, y_per_part, FrameRate):\n",
    "    Nframes = x_per_part.shape[1]\n",
    "    Npart = x_per_part.shape[0]\n",
    "    maxdt = 10\n",
    "    time = np.arange(0,maxdt+1,1)/FrameRate\n",
    "    # loop over all the particles\n",
    "    DC_part = np.zeros(Npart)\n",
    "    for i in range(Npart):\n",
    "        x = x_per_part[i,:]\n",
    "        y = y_per_part[i,:]\n",
    "        MSD = np.zeros(maxdt+1)\n",
    "        for dt in range(maxdt):\n",
    "            SD = np.square(x[1+dt:]-x[:-dt-1])+np.square(y[1+dt:]-y[:-dt-1])\n",
    "            MSD[dt+1] = np.mean(SD)\n",
    "        # apply a fit to the MSD values\n",
    "        # use linear regression for fast result.\n",
    "        reg = LinearRegression(fit_intercept=True).fit(time.reshape(-1,1),MSD.reshape(-1,1))\n",
    "        # reg.intercept_ -> if you want to obtain the intercept.\n",
    "        DC_part[i] = reg.coef_[0][0]/4\n",
    "        \n",
    "    DC_threshold_freely_diff = np.mean(DC_part)\n",
    "    freely_boolean = np.logical_and(DC_part>DC_threshold_freely_diff, act01==0)\n",
    "    freely_ind = np.where(freely_boolean)[0]\n",
    "    return freely_ind\n",
    "    \n",
    "# find stuck particles similarly to Alissa script.\n",
    "# Fit the MSD of the whole trace -> determine diffusion coefficient -> particle=stuck, if DC<threshold.\n",
    "def find_stuck_particles(x_per_part, y_per_part, FrameRate, MVbound_threshold):\n",
    "    Nframes = x_per_part.shape[1]\n",
    "    Npart = x_per_part.shape[0]\n",
    "    maxdt = 10\n",
    "    time = np.arange(0,maxdt+1,1)/FrameRate\n",
    "    # loop over all the particles\n",
    "    DC_part = np.zeros(Npart)\n",
    "    for i in range(Npart):\n",
    "        x = x_per_part[i,:]\n",
    "        y = y_per_part[i,:]\n",
    "        MSD = np.zeros(maxdt+1)\n",
    "        for dt in range(maxdt):\n",
    "            SD = np.square(x[1+dt:]-x[:-dt-1])+np.square(y[1+dt:]-y[:-dt-1])\n",
    "            MSD[dt+1] = np.mean(SD)\n",
    "        # apply a fit to the MSD values\n",
    "        # use linear regression for fast result.\n",
    "        reg = LinearRegression(fit_intercept=True).fit(time.reshape(-1,1),MSD.reshape(-1,1))\n",
    "        # reg.intercept_ -> if you want to obtain the intercept.\n",
    "        DC_part[i] = reg.coef_[0][0]/4\n",
    "\n",
    "    # Obtain the indexes from the particles for which the avg. diffusion coefficient is smaller than the threshold.\n",
    "    stuck_ind = np.where(DC_part<MVbound_threshold)[0]\n",
    "    return stuck_ind\n",
    "    \n",
    "# Function for obtaining the mean and standard deviation of fitting a poisson distribution.\n",
    "def obtain_poisson_fit_stat(events_per_particle):\n",
    "    N_particles = events_per_particle.shape[0]\n",
    "    # Using MLE to fit a poisson distribution -> parameter lambda is equal to the mean of the observations, the variance of a poisson distribution is equal to lambda as well.\n",
    "    prm_pois = np.mean(events_per_particle)\n",
    "    std_pois = np.sqrt(prm_pois)\n",
    "    SE_pois = std_pois/np.sqrt(N_particles)\n",
    "    return [prm_pois, std_pois, SE_pois]\n",
    "\n",
    "# Function for obtaining the mean and standard deviation of fitting a normal distribution.\n",
    "def obtain_normal_fit_stat(events_per_particle):\n",
    "    N_particles = events_per_particle.shape[0]\n",
    "    # Using MLE to fit a normal distribution -> prms mean&std are simply the mean and std of the observations.\n",
    "    mean_normal = np.mean(events_per_particle) \n",
    "    std_normal = np.std(events_per_particle)\n",
    "    SE_normal = std_normal/np.sqrt(N_particles)\n",
    "    return [mean_normal, std_normal, SE_normal]\n",
    "    \n",
    "# Calculate activity function for the filtering.\n",
    "def calc_activity_of_particles(pred_state_all):\n",
    "    analyzed_particles = pred_state_all.shape[0]\n",
    "    act01 = np.zeros(analyzed_particles)\n",
    "    act12 = np.zeros(analyzed_particles)\n",
    "    acttot = np.zeros(analyzed_particles)\n",
    "    for i in range(analyzed_particles):\n",
    "        [activity01, activity12, activitytotal] = calculate_activity_per_particle(pred_state_all[i,:])\n",
    "        act01[i] = activity01\n",
    "        act12[i] = activity12\n",
    "        acttot[i] = activitytotal\n",
    "    return [act01, act12, acttot]\n",
    "\n",
    "## Filter out particles that have an unusually large activity -> same removal method as Alissa uses. \n",
    "\n",
    "# Iterate until no particles are filtered out anymore.\n",
    "# Apply the filtering for both the 01 activity and the 12 activity?\n",
    "# Or apply the function on both the 01 activity and the 12 activity seperately.\n",
    "def filter_large_activity(events_per_particle, stuck_particle_ind,distribution_fit,sigma):\n",
    "    # Input: both np.arrays\n",
    "    \n",
    "    # Need to take out the stuck particles from the analysis, but keep the array the same, to obtain the correct particle index.\n",
    "    # create boolean array to indicate particles to be removed.\n",
    "    particles_stuck = np.zeros(events_per_particle.shape[0])\n",
    "    particles_stuck[stuck_particle_ind]=1\n",
    "    particles_to_remove = np.zeros(events_per_particle.shape[0])\n",
    "    stop_prm = 0\n",
    "    \n",
    "    if distribution_fit=='poisson':\n",
    "        print('The events are filtered using the Poisson distribution.')\n",
    "        while stop_prm!=1: \n",
    "            # calculate number of particles removed in previous cycle.\n",
    "            N_removed_old = np.sum(particles_to_remove)\n",
    "            # Remove the stuck particles & filtered particles and apply filtering again.\n",
    "            particles_to_analyze = events_per_particle[np.logical_and(particles_stuck==0, particles_to_remove==0)]\n",
    "            \n",
    "            if particles_to_analyze.shape[0]>0:\n",
    "                # Fit poisson and calculate the mean, std and SE of the indicated particles.\n",
    "                [events_mean, events_std, events_SE] = obtain_poisson_fit_stat(particles_to_analyze)\n",
    "                particles_to_remove = events_per_particle>events_mean+sigma*events_std\n",
    "\n",
    "            if np.sum(particles_to_remove)==N_removed_old or particles_to_analyze.shape[0]==0:\n",
    "                # no new particles are removed -> stop the while loop.\n",
    "                stop_prm=1\n",
    "            \n",
    "    elif distribution_fit=='normal':\n",
    "        print('The events are filtered using the Normal distribution.') \n",
    "        while stop_prm!=1: \n",
    "            # calculate number of particles removed in previous cycle.\n",
    "            N_removed_old = np.sum(particles_to_remove)\n",
    "            # Remove the stuck particles & filtered particles and apply filtering again.\n",
    "            particles_to_analyze = events_per_particle[np.logical_and(particles_stuck==0, particles_to_remove==0)]\n",
    "            \n",
    "            if particles_to_analyze.shape[0]>0:\n",
    "                # Fit poisson and calculate the mean, std and SE of the indicated particles.\n",
    "                [events_mean, events_std, events_SE] = obtain_normal_fit_stat(particles_to_analyze)\n",
    "                particles_to_remove = events_per_particle>events_mean+sigma*events_std\n",
    "                \n",
    "            if np.sum(particles_to_remove)==N_removed_old or particles_to_analyze.shape[0]==0:\n",
    "                # no new particles are removed -> stop the while loop.\n",
    "                stop_prm=1\n",
    "    else:\n",
    "        print('The indicated distribution_fit is not a valid input. Input options: poisson or normal.')\n",
    "    \n",
    "    filtered_particles = np.where(particles_to_remove)[0]\n",
    "    return filtered_particles\n",
    "\n",
    "def plot_diff_coef_hist(diff_coef_all, particle_index, incl_or_excl,upper_threshold,save, name):\n",
    "    diff_coef_all_arr = np.array([])\n",
    "    for i in range(diff_coef_all.shape[0]):\n",
    "        if (i in particle_index and incl_or_excl==0) or (i not in particle_index and incl_or_excl==1):\n",
    "            diff_coef_all_arr = np.append(diff_coef_all_arr, diff_coef_all[i])\n",
    "    fig, ax = plt.subplots(1,1, constrained_layout=True)\n",
    "    plt.hist(diff_coef_all_arr[diff_coef_all_arr<upper_threshold], bins=200)\n",
    "    #plt.title(\"Diffusion coefficient distribution of selected particles\")\n",
    "    plt.rc('font', size=14)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
    "    plt.xlim(0,upper_threshold)\n",
    "    plt.ylim(0,100000)\n",
    "    #ax.set_yscale('linear')\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "    ax.yaxis.major.formatter._useMathText = True\n",
    "    ax.set_xlabel(\"D ($\\u03BCm^2/s$)\")\n",
    "    plt.xticks([0,0.02,0.04,0.06,0.08,0.10],['0','0.02','0.04','0.06','0.08','0.10'])\n",
    "    plt.yticks([0,20000,40000,60000,80000,100000])\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    \n",
    "    if save==1:\n",
    "        plt.savefig(name+'\\diffusion_all')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_diff_coef_hist_per_pred_state(diff_coef_all, pred_state_all, particle_index, incl_or_excl, N_val_D, upper_threshold,save,name):\n",
    "    diff_coef_unb = np.array([])\n",
    "    diff_coef_b1 = np.array([])\n",
    "    diff_coef_b2 = np.array([])\n",
    "    for i in range(pred_state_all.shape[0]):\n",
    "        if (i in particle_index and incl_or_excl==0) or (i not in particle_index and incl_or_excl==1):\n",
    "            diff_coef_unb = np.append(diff_coef_unb, diff_coef_all[i][pred_state_all[i][np.floor(N_val_D/2).astype(int)-1:-np.floor(N_val_D/2).astype(int)]==0])\n",
    "            diff_coef_b1 = np.append(diff_coef_b1, diff_coef_all[i][pred_state_all[i][np.floor(N_val_D/2).astype(int)-1:-np.floor(N_val_D/2).astype(int)]==1])\n",
    "            diff_coef_b2 = np.append(diff_coef_b2, diff_coef_all[i][pred_state_all[i][np.floor(N_val_D/2).astype(int)-1:-np.floor(N_val_D/2).astype(int)]==2])\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,constrained_layout=True)\n",
    "    plt.hist(diff_coef_unb[diff_coef_unb<upper_threshold], bins=200, color='tab:blue', alpha=0.5, label='unbound')\n",
    "    plt.hist(diff_coef_b1[diff_coef_b1<upper_threshold], bins=200, color='green', alpha=0.5, label='single bound')\n",
    "    plt.hist(diff_coef_b2[diff_coef_b2<upper_threshold], bins=200, color='red', alpha=0.5, label='double bound')\n",
    "    #plt.title(\"Seperate diffusion coefficient distributions of selected particles\")\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "    plt.rc('font', size=14)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
    "    ax.set_xlim(0,upper_threshold)\n",
    "    ax.set_ylim(0,100000)\n",
    "    #ax.set_yscale('linear')\n",
    "    ax.yaxis.major.formatter._useMathText = True\n",
    "    ax.set_xlabel(\"D ($\\u03BCm^2/s$)\")\n",
    "    #ax.set_xticklabels([0.00],['0'])\n",
    "    plt.xticks([0,0.02,0.04,0.06,0.08,0.10],['0','0.02','0.04','0.06','0.08','0.10'])\n",
    "    plt.yticks([0,20000,40000,60000,80000,100000])\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "#     plt.rcParams['axes.linewidth'] = 0.9\n",
    "    #plt.tick_params(width=2)\n",
    "#     ax.tick_params(width=2)\n",
    "#     plt.rcParams['xtick.major.width'] = 2\n",
    "#     plt.rcParams['ytick.major.width'] = 2\n",
    "\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.xlim(0,0.15)\n",
    "    if save==1:\n",
    "        plt.savefig(name+'\\diffusion_per_state')\n",
    "    plt.show()\n",
    "    \n",
    "def lifetimes(state_arr, exclude_censored_lifetimes): #Calculates the lifetimes of a given state array\n",
    "    bound2_lifetimes=np.array([])\n",
    "    bound_lifetimes=np.array([])\n",
    "    unbound_lifetimes=np.array([])\n",
    "    i=0\n",
    "    while i<state_arr.shape[0]-1:\n",
    "        cur_state=state_arr[i]\n",
    "        lifetime=0\n",
    "        while cur_state==state_arr[i] and i<state_arr.shape[0]-1:\n",
    "            lifetime=lifetime+1\n",
    "            i=i+1\n",
    "        if cur_state==0:\n",
    "            unbound_lifetimes=np.append(unbound_lifetimes,lifetime)\n",
    "        elif cur_state==1:\n",
    "            bound_lifetimes=np.append(bound_lifetimes, lifetime)\n",
    "        else:\n",
    "            bound2_lifetimes=np.append(bound2_lifetimes, lifetime)\n",
    "            \n",
    "    if exclude_censored_lifetimes==1:\n",
    "        if state_arr[0]==0:\n",
    "            unbound_lifetimes=unbound_lifetimes[1:]\n",
    "        elif state_arr[0]==1:\n",
    "            bound_lifetimes=bound_lifetimes[1:]\n",
    "        elif state_arr[0]==2:\n",
    "            bound2_lifetimes=bound2_lifetimes[1:]\n",
    "            \n",
    "        if state_arr[state_arr.shape[0]-1]==0:\n",
    "            unbound_lifetimes=unbound_lifetimes[:-1]\n",
    "        elif state_arr[state_arr.shape[0]-1]==1:\n",
    "            bound_lifetimes = bound_lifetimes[:-1]\n",
    "        elif state_arr[state_arr.shape[0]-1]==2:\n",
    "            bound2_lifetimes=bound2_lifetimes[:-1]\n",
    "            \n",
    "    lifetimes=[unbound_lifetimes, bound_lifetimes, bound2_lifetimes]\n",
    "    return lifetimes\n",
    "\n",
    "def lifetimes_bound(state_arr2, exclude_censored_lifetimes): #Calculates the lifetimes of a given state array\n",
    "    bound2_lifetimes=np.array([])\n",
    "    bound_lifetimes=np.array([])\n",
    "    unbound_lifetimes=np.array([])\n",
    "    i=0\n",
    "    state_arr=state_arr2.copy()\n",
    "    state_arr[state_arr>1]=1\n",
    "    while i<state_arr.shape[0]-1:\n",
    "        cur_state=state_arr[i]\n",
    "        lifetime=0\n",
    "        while cur_state==state_arr[i] and i<state_arr.shape[0]-1:\n",
    "            lifetime=lifetime+1\n",
    "            i=i+1\n",
    "        if cur_state==1:\n",
    "            bound_lifetimes=np.append(bound_lifetimes, lifetime)\n",
    "    if exclude_censored_lifetimes==1:\n",
    "        if state_arr[0]==1:\n",
    "            bound_lifetimes = bound_lifetimes[1:]\n",
    "            \n",
    "        if state_arr[state_arr.shape[0]-1]==1:\n",
    "            bound_lifetimes = bound_lifetimes[:-1]\n",
    "    return bound_lifetimes\n",
    "\n",
    "def obtain_lifetimes_from_all_states(pred_state_all, particle_index,incl_or_excl,exclude_censored_lifetimes):\n",
    "    #if incl_or_excl==0:\n",
    "        #print(\"Selected particles are INCLUDED in analysis.\")\n",
    "    #elif incl_or_excl==1:\n",
    "        #print(\"Selected particles are EXCLUDED from analysis.\")\n",
    "    #else:\n",
    "    #    print(\"Incl_or_excl does not has a valid input value.\")\n",
    "    bound2_lifetimes=np.array([])\n",
    "    bound1_lifetimes=np.array([])\n",
    "    unbound_lifetimes=np.array([])\n",
    "    bound_lifetimes=np.array([])\n",
    "    analyzed_particles = 0\n",
    "    for i in range(pred_state_all.shape[0]):\n",
    "        if (i in particle_index and incl_or_excl==0) or (i not in particle_index and incl_or_excl==1):\n",
    "            LT = lifetimes(pred_state_all[i,:],exclude_censored_lifetimes)\n",
    "            unbound_lifetimes=np.append(unbound_lifetimes,LT[0])\n",
    "            bound1_lifetimes=np.append(bound1_lifetimes,LT[1])\n",
    "            bound2_lifetimes=np.append(bound2_lifetimes,LT[2])\n",
    "            bound_lifetimes=np.append(bound_lifetimes, lifetimes_bound(pred_state_all[i,:], exclude_censored_lifetimes))\n",
    "            analyzed_particles+=1\n",
    "\n",
    "    print('Number of analyzed particles:%1.0f' %analyzed_particles)\n",
    "    return [unbound_lifetimes, bound_lifetimes, bound1_lifetimes, bound2_lifetimes]\n",
    "\n",
    "def save_lifetimes_to_csv(unbound_LT, bound_LT, b1_LT, b2_LT, FrameRate,filepath_name):\n",
    "    max_length = max(unbound_LT.shape[0], bound_LT.shape[0], b1_LT.shape[0], b2_LT.shape[0])\n",
    "    unbound_list = np.zeros(max_length)\n",
    "    bound_list = np.zeros(max_length)\n",
    "    bound1_list = np.zeros(max_length)\n",
    "    bound2_list = np.zeros(max_length)\n",
    "    unbound_list[:unbound_LT.shape[0]]=unbound_LT/FrameRate\n",
    "    bound_list[:bound_LT.shape[0]]=bound_LT/FrameRate\n",
    "    bound1_list[:b1_LT.shape[0]]=b1_LT/FrameRate\n",
    "    bound2_list[:b2_LT.shape[0]]=b2_LT/FrameRate\n",
    "\n",
    "    d = {'unbound lifetimes': unbound_list.tolist(), 'bound lifetimes': bound_list.tolist(), 'bound1 lifetimes': bound1_list.tolist(),'bound2 lifetimes': bound2_list.tolist()}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv(filepath_name + \"\\ \" + filepath_name +'lifetimes.csv', index = False)\n",
    "    print(\"The lifetimes are saved to .csv -> note that the 0's are supposed to be empty values and are of course not actual lifetimes.\")\n",
    "\n",
    "# Necessary functions for plotting the lifetimes.\n",
    "def exp_lifetimes(x,a,b):\n",
    "    output=a*np.exp(-b*x)\n",
    "    return output \n",
    "\n",
    "def exp_lifetimes2(x,a,b,c,d):\n",
    "    output=a*np.exp(-b*x)+((1-a-c)*np.exp(-d*x))\n",
    "    return output \n",
    "\n",
    "def ecdf(sample):\n",
    "    # convert sample to a numpy array, if it isn't already\n",
    "    sample = np.atleast_1d(sample)\n",
    "\n",
    "    # find the unique values and their corresponding counts\n",
    "    quantiles, counts = np.unique(sample, return_counts=True)\n",
    "\n",
    "    # take the cumulative sum of the counts and divide by the sample size to\n",
    "    # get the cumulative probabilities between 0 and 1\n",
    "    cumprob = np.cumsum(counts).astype(np.double) / sample.size\n",
    "\n",
    "    return quantiles, cumprob\n",
    "\n",
    "# Function to plot and save the lifetime -> single exponential and double exponential.\n",
    "def plot_and_save_lifetime_single_expfit(lifetimes_Nframes, name, FrameRate, save, filepath_name):\n",
    "    try:\n",
    "        tau, f = ecdf(lifetimes_Nframes/FrameRate)\n",
    "        param_bounds=([0,0],[1,np.inf])\n",
    "        pred=scipy.optimize.curve_fit(exp_lifetimes,tau,1-f, np.array([0.1, 0.1]), bounds=param_bounds)\n",
    "        tau_bound1 = 1/pred[0][1]\n",
    "        lifetimes_fit=exp_lifetimes(tau,pred[0][0],pred[0][1])\n",
    "\n",
    "        # save to CSV file\n",
    "        d = {'time': tau, '1-f': 1-f, 'exp1_fit': exp_lifetimes(tau,pred[0][0],pred[0][1])}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df.to_csv(filepath_name + \"\\ \" + filepath_name + name + ' CDF and fit.csv', index = False)    \n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1, 1, constrained_layout=True)\n",
    "        plt.rc('font', size=14)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
    "        ax.scatter(tau, 1-f, lw=2,color='red', s=1)\n",
    "    #     ax.scatter(tau, 1-f, lw=2, label='Empirical CDF',color='red', s=1)\n",
    "        ax.plot(tau, exp_lifetimes(tau,pred[0][0],pred[0][1]))\n",
    "        ax.set_xlabel('Seconds')\n",
    "        ax.set_ylabel('Survival (1-CDF)')\n",
    "        ax.set_yscale('log')\n",
    "        #ax.legend(fancybox=True, loc='right')\n",
    "        ax.set_title(name +'  '+ r'$\\tau$' '=  %1.3f' %tau_bound1)\n",
    "        ax.set_ylim(0.01,1)\n",
    "\n",
    "        if save==1:\n",
    "            plt.savefig(filepath_name + \"\\ \" + filepath_name + name)\n",
    "        plt.show()\n",
    "    except:\n",
    "        tau_bound1=0\n",
    "    return tau_bound1\n",
    "    \n",
    "def plot_and_save_lifetime_double_expfit(lifetimes_Nframes, name, FrameRate, save, filepath_name):\n",
    "    try:\n",
    "        tau, f = ecdf(lifetimes_Nframes/FrameRate)\n",
    "        param_bounds=([0,0,0,0],[1,np.inf,1,np.inf])\n",
    "        pred=scipy.optimize.curve_fit(exp_lifetimes2,tau,1-f, np.array([0.1, 0.1,0.1,0.001]), bounds=param_bounds)\n",
    "        tau_1 = 1/pred[0][1]\n",
    "        tau_2 = 1/pred[0][3]\n",
    "        fract1 = pred[0][0]\n",
    "        fract2 = 1-pred[0][0]-pred[0][2]\n",
    "        lifetimes_fit=exp_lifetimes2(tau,pred[0][0],pred[0][1],pred[0][2],pred[0][3])\n",
    "\n",
    "        # save to CSV file\n",
    "        d = {'time': tau, '1-f': 1-f, 'exp2_fit': lifetimes_fit}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df.to_csv(filepath_name + \"\\ \" + filepath_name + name + ' CDF and fit.csv', index = False)        \n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1, 1, constrained_layout=True)\n",
    "        plt.rc('font', size=14)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=14)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
    "    #     ax.scatter(tau, 1-f, lw=2, label='Empirical CDF',color='red', s=1)\n",
    "        ax.scatter(tau, 1-f, lw=2, color='red', s=1)\n",
    "        ax.plot(tau, exp_lifetimes2(tau,pred[0][0],pred[0][1],pred[0][2],pred[0][3]))\n",
    "        ax.set_xlabel('Seconds')\n",
    "        ax.set_ylabel('Survival (1-CDF)')\n",
    "        ax.set_yscale('log')\n",
    "        #ax.legend(fancybox=True, loc='right')\n",
    "        ax.set_title(name + '  ' + r'$\\tau_1$' '=  %1.3f' %tau_1 +  ' , '+ r'$\\tau_2$' '=  %1.3f' %tau_2 + '\\n fraction1 = %1.3f' %fract1 + ', fraction2 = %1.3f' %fract2)\n",
    "        ax.set_ylim(0.01,1)    \n",
    "        if save==1:\n",
    "            plt.savefig(filepath_name + \"\\ \" + filepath_name + name)\n",
    "        plt.show()\n",
    "    except:\n",
    "        tau_1=0\n",
    "        tau_2=0\n",
    "        fract1=0\n",
    "        fract2=0\n",
    "    return [tau_1, fract1, tau_2, fract2]\n",
    "    \n",
    "    \n",
    "# Necessary functions for calculating the bound fractions.\n",
    "def calculate_boundfraction_per_particle(pred_states):\n",
    "    unb_fr = np.sum(pred_states==0)/pred_states.shape[0]\n",
    "    b1_fr = np.sum(pred_states==1)/pred_states.shape[0]\n",
    "    b2_fr = np.sum(pred_states==2)/pred_states.shape[0]\n",
    "    btot_fr = b1_fr+b2_fr\n",
    "    return [unb_fr, b1_fr, b2_fr, btot_fr]\n",
    "\n",
    "def save_boundfraction_of_selected_particles(pred_state_all, particle_index, incl_or_excl, name,save):\n",
    "    if incl_or_excl==0:\n",
    "        #print(\"Selected particles are INCLUDED in analysis.\")\n",
    "        analyzed_particles = len(particle_index)\n",
    "    elif incl_or_excl==1:\n",
    "        #print(\"Selected particles are EXCLUDED from analysis.\")\n",
    "        analyzed_particles = pred_state_all.shape[0]-len(particle_index)\n",
    "    else:\n",
    "        print(\"Incl_or_excl does not has a valid input value.\")\n",
    "    \n",
    "    particle_number = np.zeros(analyzed_particles)\n",
    "    ub_fr_arr = np.zeros(analyzed_particles)\n",
    "    b1_fr_arr = np.zeros(analyzed_particles)\n",
    "    b2_fr_arr = np.zeros(analyzed_particles)\n",
    "    btot_fr_arr = np.zeros(analyzed_particles)\n",
    "    k=0\n",
    "    for i in range(pred_state_all.shape[0]):\n",
    "        if (i in particle_index and incl_or_excl==0) or (i not in particle_index and incl_or_excl==1):\n",
    "            [unb_fr, b1_fr, b2_fr, btot_fr] = calculate_boundfraction_per_particle(pred_state_all[i,:])\n",
    "            ub_fr_arr[k] = unb_fr\n",
    "            b1_fr_arr[k] = b1_fr\n",
    "            b2_fr_arr[k] = b2_fr\n",
    "            btot_fr_arr[k] = btot_fr\n",
    "            particle_number[k] = i \n",
    "            k+=1\n",
    "            \n",
    "    part_num_list=particle_number.tolist()\n",
    "    part_num_list.insert(0,'Total')\n",
    "    unb_fr_list = ub_fr_arr.tolist()\n",
    "    unb_fr_list.insert(0,np.mean(ub_fr_arr))\n",
    "    b1_fr_list=b1_fr_arr.tolist()\n",
    "    b1_fr_list.insert(0,np.mean(b1_fr_arr))\n",
    "    b2_fr_list=b2_fr_arr.tolist()\n",
    "    b2_fr_list.insert(0,np.mean(b2_fr_arr))\n",
    "    btot_fr_list=btot_fr_arr.tolist()\n",
    "    btot_fr_list.insert(0,np.mean(btot_fr_arr))\n",
    "    \n",
    "    if save==1:\n",
    "        d = {'particle number': part_num_list, 'unbound fraction': unb_fr_list, 'single bound fraction': b1_fr_list, 'double bound fraction': b2_fr_list, 'bound fraction': btot_fr_list}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df.to_csv(name+\"\\ \"+'boundfractions.csv', index = False)\n",
    "\n",
    "    print('Number of analyzed particles:%1.0f' %analyzed_particles)\n",
    "    print(\"The total unbound/single bound/double bound fractions of the selected particles is equal to %1.3f\" %np.mean(ub_fr_arr)+ \"/%1.3f\" %np.mean(b1_fr_arr) + \"/%1.3f\" %np.mean(b2_fr_arr))\n",
    "    return [np.mean(ub_fr_arr),np.mean(b1_fr_arr),np.mean(b2_fr_arr)]\n",
    "\n",
    "# Necessary functions for calculating the activity of specific particles.\n",
    "def calculate_activity_per_particle(state_arr):\n",
    "    activity01=0\n",
    "    activity12=0\n",
    "    activitytotal=0\n",
    "    i=0\n",
    "    prev_state=state_arr[i]\n",
    "    while i<state_arr.shape[0]-1:\n",
    "        cur_state=state_arr[i]\n",
    "        if cur_state!=prev_state:\n",
    "            if prev_state==0:\n",
    "                if cur_state==1:\n",
    "                    activity01+=1\n",
    "                    activitytotal+=1\n",
    "                elif cur_state==2:\n",
    "                    activity01+=1\n",
    "                    activity12+=1\n",
    "                    activitytotal+=2\n",
    "            elif prev_state==1:\n",
    "                if cur_state==0:\n",
    "                    activity01+=1\n",
    "                    activitytotal+=1\n",
    "                elif cur_state==2:\n",
    "                    activity12+=1\n",
    "                    activitytotal+=1\n",
    "            elif prev_state==2:\n",
    "                if cur_state==0:\n",
    "                    activity01+=1\n",
    "                    activity12+=1\n",
    "                    activitytotal+=2\n",
    "                elif cur_state==1:\n",
    "                    activity12+=1\n",
    "                    activitytotal+=1\n",
    "        prev_state=cur_state\n",
    "        i+=1\n",
    "    return [activity01, activity12, activitytotal]\n",
    "    \n",
    "\n",
    "def save_activity_of_selected_particles(pred_state_all, particle_index, incl_or_excl,N_frames,FrameRate,name,save):\n",
    "    if incl_or_excl==0:\n",
    "        #print(\"Selected particles are INCLUDED in analysis.\")\n",
    "        analyzed_particles = len(particle_index)\n",
    "    elif incl_or_excl==1:\n",
    "        #print(\"Selected particles are EXCLUDED from analysis.\")\n",
    "        analyzed_particles = pred_state_all.shape[0]-len(particle_index)\n",
    "    else:\n",
    "        print(\"Incl_or_excl does not have a valid input value.\")\n",
    "        \n",
    "    particle_number = np.zeros(analyzed_particles)\n",
    "    act01 = np.zeros(analyzed_particles)\n",
    "    act12 = np.zeros(analyzed_particles)\n",
    "    acttot = np.zeros(analyzed_particles)\n",
    "    k=0\n",
    "    for i in range(pred_state_all.shape[0]):\n",
    "        if (i in particle_index and incl_or_excl==0) or (i not in particle_index and incl_or_excl==1):\n",
    "            [activity01, activity12, activitytotal] = calculate_activity_per_particle(pred_state_all[i,:])\n",
    "            act01[k] = activity01\n",
    "            act12[k] = activity12\n",
    "            acttot[k] = activitytotal\n",
    "            particle_number[k] = i \n",
    "            k+=1\n",
    "            \n",
    "    part_num_list=particle_number.tolist()\n",
    "    part_num_list.insert(0,'Mean Activity')\n",
    "    part_num_list.insert(0,'Total Activity')\n",
    "    part_num_list.insert(0,'Activity mHz')\n",
    "    \n",
    "    act01_list = act01.tolist()\n",
    "    act01_list.insert(0,np.mean(act01))\n",
    "    act01_list.insert(0,np.sum(act01))\n",
    "    act01_list.insert(0,np.mean(act01)/(np.floor(N_frames/FrameRate))*1000)\n",
    "    \n",
    "    act12_list = act12.tolist() \n",
    "    act12_list.insert(0,np.mean(act12))\n",
    "    act12_list.insert(0,np.sum(act12))\n",
    "    act12_list.insert(0,np.mean(act12)/(np.floor(N_frames/FrameRate))*1000)\n",
    "    \n",
    "    acttot_list=acttot.tolist()\n",
    "    acttot_list.insert(0,np.mean(acttot))\n",
    "    acttot_list.insert(0,np.sum(acttot))\n",
    "    acttot_list.insert(0,np.mean(acttot)/(np.floor(N_frames/FrameRate))*1000)\n",
    "    \n",
    "    if save==1:\n",
    "        d = {'particle number': part_num_list, '01 Activity': act01_list, '12 Activity': act12_list, 'Total Activity': acttot_list}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df.to_csv(name+\"\\ \"+'activity.csv', index = False)\n",
    "\n",
    "    print('Number of analyzed particles:%1.0f' %analyzed_particles)\n",
    "    print(\"The total and mean 01 activity selected particles is equal to %1.3f\" %np.sum(act01)+ \"/%1.3f\" %np.mean(act01))\n",
    "    print(\"The total and mean 12 activity selected particles is equal to %1.3f\" %np.sum(act12)+ \"/%1.3f\" %np.mean(act12))\n",
    "    print(\"The total and mean activity selected particles is equal to %1.3f\" %np.sum(acttot)+ \"/%1.3f\" %np.mean(acttot))\n",
    "    return [np.mean(act01), np.mean(act12), np.mean(acttot)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to do the whole analysis at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that obtains all the .txt files in the directory.\n",
    "def obtain_txt_file_names_in_dir():\n",
    "    namelist = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".txt\"):\n",
    "            namelist.append(os.path.splitext(filename)[0])\n",
    "    return namelist\n",
    "\n",
    "# Function that performs the whole deep learning analysis.\n",
    "def perform_deep_learning_analysis(PixelSize, FrameRate, model01, model12, name, measurementwindow1, measurementwindow2, MVbound_threshold, sigma, distribution_fit, exclude_censored_lifetimes):\n",
    "    os.mkdir(name)\n",
    "    print(name)\n",
    "    data = np.genfromtxt(name+\".txt\", delimiter=',')\n",
    "    # Scale the coordinates with the pixelsize.\n",
    "    xy_list = data[:,:-1]*PixelSize\n",
    "    num_particles = data.shape[0]\n",
    "    print('The total number of particles is equal to %1.0f' %num_particles)\n",
    "\n",
    "    # Obtain xy coordinates for each particle\n",
    "    x_per_part=obtain_xy_per_particle(xy_list)[0]\n",
    "    y_per_part=obtain_xy_per_particle(xy_list)[1]\n",
    "    N_frames = x_per_part.shape[1]\n",
    "    print('The number of frames per particle is equal to %1.0f' %N_frames + ', which corresponds to %1.0f seconds.' %np.floor(N_frames/FrameRate))\n",
    "\n",
    "    # Function to downscale the FrameRate from 60 to 30 -> this allows the deep learning algorithm to work faster (at similar performance)\n",
    "    # If this function is used -> also need to use the deep learning model for 30 FrameRate, rather than the one for 60 FrameRate. (furthermore, the variable FrameRate is automatically set to 30)\n",
    "    if FrameRate==60:\n",
    "        [x_per_part, y_per_part, N_frames, FrameRate] = downscale_FR60_to_FR30(x_per_part, y_per_part)\n",
    "\n",
    "    # Empty array to store the predicted states of all the particles.\n",
    "    pred_state_all = np.zeros((num_particles,N_frames))\n",
    "\n",
    "    no_MSD_val=10 #number of MSD values used for the diffusion coefficient calculation -> called maxdt in the matlab script.\n",
    "    N_val_DiffCoef = 100 #measurementwindow for the diffusion coefficient calculation. \n",
    "\n",
    "    diff_coef_all = np.zeros((num_particles, N_frames-N_val_DiffCoef+1)) # Empty array to store the calculated values of the diffusion coefficient. \n",
    "    Time = np.linspace(1/FrameRate, N_frames/FrameRate, N_frames)\n",
    "\n",
    "    #Normalization of the x,y values. Important for training deep learning model (goes faster when input values are of order unity). Do not change unless specified (currently not the case). \n",
    "    #Currently, the simulation is outputted in meters (norm4e-7) while the experimental data is outputted in um (therefore norm 0.4)\n",
    "    norm=0.4 \n",
    "    norm2=norm\n",
    "\n",
    "    smooth01 = measurementwindow+1\n",
    "    smooth12 = measurementwindow2+1\n",
    "    print('xy-scatter plot, blue=unbound, green=single bound, red=double bound. Diffusion coefficient plot is added next to the predicted states for reference, it is not used to determine the states.')\n",
    "    for p_num in range(num_particles):\n",
    "    #for p_num in range(5):\n",
    "        x=x_per_part[p_num,:]\n",
    "        y=y_per_part[p_num,:]\n",
    "\n",
    "        X_test = X_test_from_timeseries(x,y,measurementwindow) # Create input subtrajectories for classification. \n",
    "        for i in range(X_test.shape[0]):\n",
    "            X_test[i,:,:2]-=X_test[i,0,:2] # minus the first coordinate to start the trajectories in the same spot.\n",
    "        X_test=X_test/norm #normalize by the bound radius of the first state.\n",
    "    \n",
    "        # Obtain predictions for unbound/bound\n",
    "        Ypred01_smooth=smoother_unev(obtain_state_per_pos(model01.predict(X_test), measurementwindow),smooth01)\n",
    "        # Obtain predictions for single/double bond\n",
    "        Ypred12_smooth = obtain_pred_state2(x, y, Ypred01_smooth, measurementwindow2, norm2, model12, smooth12)\n",
    "        print(len(Ypred12_smooth))\n",
    "        #Save the predicted states for later analysis.\n",
    "        pred_state_all[p_num,:] = Ypred12_smooth\n",
    "        \n",
    "        diff_coef_all[p_num,:] = diff_coef_over_time(x,y,N_val_DiffCoef,no_MSD_val, FrameRate)\n",
    "\n",
    "        \n",
    "        ## Plot \n",
    "        if p_num<50:\n",
    "            Ypred12_smooth_plot = pred_state_all[p_num,0:len(diff_coef_all[0])]\n",
    "            Time_plot = Time[0:len(diff_coef_all[0])]\n",
    "            x=x_per_part[p_num,0:len(diff_coef_all[0])]\n",
    "            y=y_per_part[p_num,0:len(diff_coef_all[0])]\n",
    "            \n",
    "            SMALL_SIZE = 24\n",
    "            MEDIUM_SIZE = 26\n",
    "            BIGGER_SIZE = 28\n",
    "\n",
    "            ubtime=Time_plot.copy()\n",
    "            sbtime=Time_plot.copy()\n",
    "            mbtime=Time_plot.copy()\n",
    "            ubdiffcoeff=diff_coef_all.copy()\n",
    "            sbdiffcoeff=diff_coef_all.copy()\n",
    "            mbdiffcoeff=diff_coef_all.copy()\n",
    "            ubstate=Ypred12_smooth_plot.copy()\n",
    "            sbstate=Ypred12_smooth_plot.copy()\n",
    "            mbstate=Ypred12_smooth_plot.copy()\n",
    "\n",
    "            for timepoint1 in range(len(Ypred12_smooth_plot)):\n",
    "                if Ypred12_smooth_plot[timepoint1] != 0:\n",
    "                    ubtime[timepoint1]= np.nan\n",
    "                    ubdiffcoeff[p_num,timepoint1]=np.nan\n",
    "                    ubstate[timepoint1]=np.nan\n",
    "            #         display(timepoint1)\n",
    "\n",
    "            for timepoint2 in range(len(Ypred12_smooth_plot)-1):\n",
    "                if Ypred12_smooth_plot[timepoint2] != 1:\n",
    "                    sbtime[timepoint2]= np.nan\n",
    "                    sbdiffcoeff[p_num,timepoint2]=np.nan\n",
    "                    sbstate[timepoint1]=np.nan\n",
    "\n",
    "            for timepoint3 in range(len(Ypred12_smooth_plot)-1):\n",
    "                if Ypred12_smooth_plot[timepoint3] != 2:\n",
    "                    mbtime[timepoint3]= np.nan\n",
    "                    mbdiffcoeff[p_num,timepoint3]=np.nan\n",
    "                    mbstate[timepoint1]=np.nan\n",
    "            \n",
    "            particlenumber=p_num+1\n",
    "            \n",
    "            print(name + ': plots for particle %1.0f' %particlenumber)\n",
    "#             fig2, axs = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(24,8), gridspec_kw={'width_ratios': [1, 2]})\n",
    "#             axs[0].scatter(x[Ypred12_smooth==0], y[Ypred12_smooth==0], s=1)\n",
    "#             axs[0].scatter(x[Ypred12_smooth==1], y[Ypred12_smooth==1], s=2, color='green')\n",
    "#             axs[0].scatter(x[Ypred12_smooth==2], y[Ypred12_smooth==2], s=0.2, color='red')\n",
    "\n",
    "#             axs[1].plot(Time[np.floor(N_val_DiffCoef/2).astype(int)-1:-np.floor(N_val_DiffCoef/2).astype(int)], diff_coef_all[p_num,:])\n",
    "#             axs[1].plot(Time, -1*Ypred12_smooth/20)\n",
    "\n",
    "            fig2, axs = plt.subplot_mosaic([['xy', 'D'], ['xy', 'state']],constrained_layout=True,  figsize=(24,8), gridspec_kw={'width_ratios': [1, 2]})\n",
    "            axs['xy'].scatter(x[Ypred12_smooth_plot==0], y[Ypred12_smooth_plot==0], s=1)\n",
    "            axs['xy'].scatter(x[Ypred12_smooth_plot==1], y[Ypred12_smooth_plot==1], s=2, color='green')\n",
    "            axs['xy'].scatter(x[Ypred12_smooth_plot==2], y[Ypred12_smooth_plot==2], s=0.2, color='red')\n",
    "            axs['xy'].set_xlabel(\"x-coordinate (\\u03BCm)\")\n",
    "            axs['xy'].set_ylabel(\"y-coordinate (\\u03BCm)\")\n",
    "            plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "            plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "            plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "            plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "            plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "            plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "            plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "            axs['D'].plot(ubtime, ubdiffcoeff[p_num,:])\n",
    "            axs['D'].plot(sbtime, sbdiffcoeff[p_num,:], color='green')\n",
    "            axs['D'].plot(mbtime, mbdiffcoeff[p_num,:], color='red')\n",
    "            #axs['D'].set_xlabel(\"Time (s)\")\n",
    "            axs['D'].set_ylabel(\"D ($\\u03BCm^2/s$)\")\n",
    "            axs['D'].set_ylim([0,0.1])\n",
    "            axs['D'].set_xlim([0,np.floor(N_frames/FrameRate)])\n",
    "            axs['D'].set_yticks(np.arange(0,0.11,step=0.02))\n",
    "            \n",
    "            axs['state'].plot(Time_plot, -1*Ypred12_smooth_plot, 'k--',linewidth=2)\n",
    "            axs['state'].plot(ubtime, -1*ubstate,linewidth=4)\n",
    "            axs['state'].plot(sbtime, -1*sbstate, color='green', linewidth=4)\n",
    "            axs['state'].plot(mbtime, -1*mbstate, color='red',linewidth=4)\n",
    "            plt.yticks([0,-1,-2],['Unbound','Single bound','Double bound'])\n",
    "            axs['state'].set_xlabel(\"Time (s)\")\n",
    "            axs['state'].set_xlim([0,np.floor(N_frames/FrameRate)])\n",
    "            #axs['state'].set_ylabel(\"State\")\n",
    "            \n",
    "            fig2.savefig(name + \"\\ \" + name + 'Particle %1.0f' %p_num)\n",
    "\n",
    "            plt.show()\n",
    "            #plt.plot(Time,x-x[0])\n",
    "            #plt.plot(Time,y-y[0])\n",
    "            #plt.show()\n",
    "        \n",
    " \n",
    "    # Save results into specific folder\n",
    "    pd.DataFrame(pred_state_all).to_csv(name+\"\\predicted_state_all.csv\")\n",
    "    pd.DataFrame(diff_coef_all).to_csv(name+\"\\Dtotal.csv\")\n",
    "    # Remove stuck particles.\n",
    "    stuck_particle_ind = find_stuck_particles(x_per_part, y_per_part, FrameRate, MVbound_threshold)\n",
    "    print('The total number of stuck particles is equal to: %1.0f' %stuck_particle_ind.shape[0])\n",
    "\n",
    "    # Remove/Recognize mislocalized particles\n",
    "    misl_part_index = find_mislocalizations(x_per_part, y_per_part, FrameRate).astype(int)\n",
    "    print('The total number of mislocalized particles is equal to: %1.0f' %misl_part_index.shape[0])\n",
    "\n",
    "    # Filter the particles that have a large activity in total and particles that have an unusually large activity between the 01 state.\n",
    "    [act01, act12, acttot] = calc_activity_of_particles(pred_state_all)\n",
    "    filter01 = filter_large_activity(act01, stuck_particle_ind,distribution_fit,sigma)\n",
    "    # Filter the particles with high 12 activity -> but do not include particles that are only freely diffusing -> otherwise the algorithm will throw out almost all the particles with 12 activity.\n",
    "    # freely_diff_part = find_freely_diffusing_particles(act01, x_per_part, y_per_part, FrameRate)\n",
    "\n",
    "    # Filter the particles with high 12 activity -> but do not include particles that show no 12 activity at all ->otherwise the algorithm will throw out almost all the particles with any 12 activity.\n",
    "    freely_diff_part = np.where(act12==0)[0]\n",
    "    filter12 = filter_large_activity(act12, np.append(stuck_particle_ind, freely_diff_part),distribution_fit,sigma+3)\n",
    "\n",
    "    filtered_particles = np.unique(np.append(filter01, filter12))\n",
    "    print('The total number of filtered particles is equal to: %1.0f' %filtered_particles.shape[0])\n",
    "\n",
    "    remove_particle_all = np.unique(np.append(np.append(stuck_particle_ind, misl_part_index), filtered_particles))\n",
    "    \n",
    "    # Plot and save distributions etc.\n",
    "    save=1\n",
    "    particle_index= remove_particle_all #exclude the particles that are filtered out above.\n",
    "    # Indicate whether you want to include/exclude the indicated particles. To include: incl_or_excl=0. To exclude: incl_or_excl=1.\n",
    "    incl_or_excl=1\n",
    "    upper_threshold = 0.1 # do not plot values of the diffusion coefficient larger than this value.\n",
    "\n",
    "    #Plot the complete diffusion coefficient distribution\n",
    "    plot_diff_coef_hist(diff_coef_all, particle_index, incl_or_excl,upper_threshold,save,name)\n",
    "\n",
    "    #Plot the diffusion coefficient distributions of unbound, single and double bound seperately.\n",
    "    plot_diff_coef_hist_per_pred_state(diff_coef_all, pred_state_all, particle_index, incl_or_excl, N_val_DiffCoef, upper_threshold,save,name)\n",
    "    \n",
    "    # Determine Lifetimes per particle state trace\n",
    "    [unbound_lifetimes_Nframes, bound_lifetimes_Nframes, bound1_lifetimes_Nframes, bound2_lifetimes_Nframes] = obtain_lifetimes_from_all_states(pred_state_all, particle_index, incl_or_excl,exclude_censored_lifetimes)\n",
    "    \n",
    "    tau_unb_single = plot_and_save_lifetime_single_expfit(unbound_lifetimes_Nframes, 'unbound lifetime exp1', FrameRate,save,name)\n",
    "    [tau_1_unb, fract1_unb, tau_2_unb, fract2_unb] = plot_and_save_lifetime_double_expfit(unbound_lifetimes_Nframes, 'unbound lifetime exp2', FrameRate, save,name)\n",
    "    [tau_1_bound, fract1_bound, tau_2_bound, fract2_bound] = plot_and_save_lifetime_double_expfit(bound_lifetimes_Nframes, 'all bound lifetimes exp2', FrameRate,save,name)\n",
    "    tau_b1_single = plot_and_save_lifetime_single_expfit(bound1_lifetimes_Nframes, 'single bound lifetime exp1', FrameRate,save,name)\n",
    "    [tau_1_b1, fract1_b1, tau_2_b1, fract2_b1] = plot_and_save_lifetime_double_expfit(bound1_lifetimes_Nframes, 'single bound lifetime exp2', FrameRate,save,name)\n",
    "    tau_1_b2_single = plot_and_save_lifetime_single_expfit(bound2_lifetimes_Nframes, 'double bound lifetime exp1', FrameRate,save,name)\n",
    "    [tau_1_b2, fract1_b2, tau_2_b2, fract2_b2] = plot_and_save_lifetime_double_expfit(bound2_lifetimes_Nframes, 'double bound lifetime exp2', FrameRate,save,name)\n",
    "    \n",
    "    # Determine bound fraction and activity.\n",
    "    [mean_bf01, mean_bf12, mean_bftot] = save_boundfraction_of_selected_particles(pred_state_all, particle_index, incl_or_excl, name,1)\n",
    "    [mean_act01, mean_act12, mean_acttot] = save_activity_of_selected_particles(pred_state_all, particle_index, incl_or_excl,N_frames,FrameRate, name,1)\n",
    "    \n",
    "    # Functions to determine unfiltered bound fraction/activity/lifetimes of non-filtered vs. filtered. \n",
    "    [mean_bf01_unf, mean_bf12_unf, mean_bftot_unf] = save_boundfraction_of_selected_particles(pred_state_all, [], 0, name,0)\n",
    "    [mean_act01_unf, mean_act12_unf, mean_acttot_unf] = save_activity_of_selected_particles(pred_state_all, [], 0, N_frames, FrameRate, name,0)\n",
    "    \n",
    "    # Determine Lifetimes per particle state trace - unfiltered.\n",
    "    [unbound_lifetimes_Nframes, bound_lifetimes_Nframes, bound1_lifetimes_Nframes, bound2_lifetimes_Nframes] = obtain_lifetimes_from_all_states(pred_state_all, [], 1,exclude_censored_lifetimes)\n",
    "    ## Save lifetimes (in seconds) of specified particles to a csv file -> UNCOMMENT TO USE.\n",
    "    save_lifetimes_to_csv(unbound_lifetimes_Nframes, bound_lifetimes_Nframes, bound1_lifetimes_Nframes, bound2_lifetimes_Nframes, FrameRate, name)\n",
    "    \n",
    "    tau_unb_single_unf = plot_and_save_lifetime_single_expfit(unbound_lifetimes_Nframes, 'unbound lifetime exp1 - unfiltered', FrameRate,save,name)\n",
    "    [tau_1_unb_unf, fract1_unb_unf, tau_2_unb_unf, fract2_unb_unf] = plot_and_save_lifetime_double_expfit(unbound_lifetimes_Nframes, 'unbound lifetime exp2 - unfiltered', FrameRate, save,name)\n",
    "    [tau_1_bound_unf, fract1_bound_unf, tau_2_bound_unf, fract2_bound_unf] = plot_and_save_lifetime_double_expfit(bound_lifetimes_Nframes, 'bound lifetime exp2 - unfiltered', FrameRate,save,name)\n",
    "    tau_b1_single_unf = plot_and_save_lifetime_single_expfit(bound1_lifetimes_Nframes, 'single bound lifetime exp1- unfiltered', FrameRate,save,name)\n",
    "    [tau_1_b1_unf, fract1_b1_unf, tau_2_b1_unf, fract2_b1_unf] = plot_and_save_lifetime_double_expfit(bound1_lifetimes_Nframes, 'single bound lifetime exp2 - unfiltered', FrameRate,save,name)\n",
    "    tau_1_b2_single_unf = plot_and_save_lifetime_single_expfit(bound2_lifetimes_Nframes, 'double bound lifetime exp1 - unfiltered', FrameRate,save,name)\n",
    "    [tau_1_b2_unf, fract1_b2_unf, tau_2_b2_unf, fract2_b2_unf] = plot_and_save_lifetime_double_expfit(bound2_lifetimes_Nframes, 'double bound lifetime exp2 - unfiltered', FrameRate,save,name)\n",
    "    \n",
    "\n",
    "    return [mean_bf01, mean_bf12, mean_bftot, mean_act01, mean_act12, mean_acttot, mean_bf01_unf, mean_bf12_unf, mean_bftot_unf, mean_act01_unf, mean_act12_unf, mean_acttot_unf, tau_unb_single, tau_1_unb, fract1_unb, tau_2_unb, fract2_unb, tau_1_bound, fract1_bound, tau_2_bound, fract2_bound, tau_b1_single, tau_1_b1, fract1_b1, tau_2_b1, fract2_b1, tau_1_b2_single, tau_1_b2, fract1_b2, tau_2_b2, fract2_b2, tau_unb_single_unf, tau_1_unb_unf, fract1_unb_unf, tau_2_unb_unf, fract2_unb_unf, tau_1_bound_unf, fract1_bound_unf, tau_2_bound_unf, fract2_bound_unf, tau_b1_single_unf, tau_1_b1_unf, fract1_b1_unf, tau_2_b1_unf, fract2_b1_unf, tau_1_b2_single_unf, tau_1_b2_unf, fract1_b2_unf, tau_2_b2, fract2_b2_unf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Model selection\n",
    "\n",
    "### Important note #1: the PixelSize needs to be accurately filled in for each analyzed dataset.\n",
    "\n",
    "The deep learning algorithm has been trained on specific motion patterns with a certain bound radius and relative changes in x and y. Using the wrong pixelsize results in smaller/larger bound radius and smaller/larger changes in x and y coordinates. Therefore, if wrong, the algorithm will classify the trajectory more as (double) bound/unbound respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note #2\n",
    "The deep learning models have been trained on a framerate of 30.  Therefore, datasets that are measured at 60 frames/s are downscaled to 30/s automatically. \n",
    "\n",
    "Furthermore, the measurementwindow of the deep learning model must be saved in python as a parameter. The measurementwindow of the deep learning model is specified in the title. \n",
    "\n",
    "    Measurementwindow -> corresponds to the measurementwindow of model01 - the model to differentiate between unbound/bound.\n",
    "    Measurementwindow2 -> corresponds to the measurementwindow of model12 - the model to differentiate between single/double bound.\n",
    "\n",
    "The measurementwindow varies for 1 um and 3 um particles -> so check if its correctly filled in!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PixelSize=0.345; #Leica M1/M3 with grasshopper3 = 0.345 um/pixel; M2 = 0,588; correct for c-mount x1 \n",
    "FrameRate=60 # Fill in the framerate of the measurement. (not the framerate of the deep learning model)\n",
    "\n",
    "# Model to differentiate between unbound/bound.\n",
    "model01 = keras.models.load_model('DLmodel01_3um_meas120_norm4e-7_FR30.h5')\n",
    "# Model to differentiate between single/double bound.\n",
    "model12 = keras.models.load_model('DLmodel12_3um_meas180_norm4e-7_FR30.h5')\n",
    "measurementwindow = 120\n",
    "measurementwindow2 = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for automatic filtering of particles\n",
    "Firstly, the stuck particles are removed. This is done by setting a threshold on the mean diffusion coefficient of a particle. If that value is too low, the particle is classified as stuck.\n",
    "\n",
    "Furthermore, mislocalized particles are removed. These are particles which move close to ecah other & afterwards are seen as the same particle in the localization software.\n",
    "\n",
    "Lastly, the particles with an unusually large activity are removed. Namely, in the experiment, often certain particles have a much higher activity compared to the whole, these are actvity outliers and are removed. This removal is done via the same filtering method Alissa uses in her matlab script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stuck particles.\n",
    "MVbound_threshold = 0.005 # Threshold for the mean diffusion coefficient which determines wether a particle is classified as 'stuck'.\n",
    "\n",
    "# Filtering outliers\n",
    "# Indicate the sigma value you want to use for the standard deviation -> sigma indicates the number of standard deviations a particle must surpass for it to be classified as 'unusually large number of events'.\n",
    "sigma=4\n",
    "# Indicate which fittype you want to use for the filtering by uncommenting the desired distribution.\n",
    "distribution_fit = 'poisson'\n",
    "#distribution_fit = 'normal'\n",
    "\n",
    "# Indicate whether you want to exclude censored lifetimes. To exclude: exclude_censored_lifetimes=1.\n",
    "exclude_censored_lifetimes=0\n",
    "# Censored lifetimes: lifetimes that 'begin' or 'end' at the start or end of the measurement respectively, and therefore are not completely observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Analyzing All .txt Files\n",
    "Code to loop through all the .txt files.\n",
    "\n",
    "Common error code: if there is already a folder with the same name as the .txt file -> it will produce an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_txt_filenames = obtain_txt_file_names_in_dir()\n",
    "# Loop through all the .txt files in the directory.\n",
    "N_txt_files = len(all_txt_filenames)\n",
    "value_mat = np.zeros((N_txt_files, 50))\n",
    "k=0\n",
    "for name in all_txt_filenames:\n",
    "    # perform_deep_learning_analysis(PixelSize, FrameRate, model01, model12, name, measurementwindow, measurementwindow2, MVbound_threshold, sigma, distribution_fit, exclude_censored_lifetimes)\n",
    "    # 30 values as output -> save in excel file.\n",
    "    values_out = perform_deep_learning_analysis(PixelSize, FrameRate, model01, model12, name, measurementwindow, measurementwindow2, MVbound_threshold, sigma, distribution_fit, exclude_censored_lifetimes)\n",
    "    value_mat[k,:] = np.array(values_out)\n",
    "    k+=1\n",
    "    \n",
    "# Save the found values for all the txt files in a single .csv file.\n",
    "for i in range(value_mat.shape[1]):\n",
    "    list_i = value_mat[:,i].tolist()\n",
    "    exec(f'csv_{i+1} = list_i')\n",
    "\n",
    "d = {'name txt file': all_txt_filenames, 'Mean Unbound Fraction Filtered': csv_1, 'Mean b1 Fraction Filtered': csv_2, 'Mean b2 Fraction Filtered': csv_3, 'Mean 01 Activity Filtered': csv_4, 'Mean 12 Activity Filtered': csv_5, 'Mean Total Activity Filtered': csv_6, 'Mean Unbound Fraction Unfiltered': csv_7, 'Mean b1 Fraction Unfiltered': csv_8, 'Mean b2 Fraction Unfiltered': csv_9, 'Mean 01 Activity Unfiltered': csv_10, 'Mean 12 Activity Unfiltered': csv_11, 'Mean Total Activity Unfiltered': csv_12, 'Unbound Lifetime Filtered - Single exp. fit': csv_13, 'Unbound Lifetime Filtered - Double exp. fit - tau1': csv_14, 'Unbound Lifetime Filtered - Double exp. fit - fraction1': csv_15, 'Unbound Lifetime Filtered - Double exp. fit - tau2': csv_16, 'Unbound Lifetime Filtered - Double exp. fit - fraction2': csv_17, 'Bound Lifetime Filtered - Double exp. fit - tau1': csv_18, 'Bound Lifetime Filtered - Double exp. fit - fraction1': csv_19, 'Bound Lifetime Filtered - Double exp. fit - tau2': csv_20, 'Bound Lifetime Filtered - Double exp. fit - fraction2': csv_21, 'Single Bound Lifetime Filtered - Single exp. fit': csv_22, 'Single Bound Lifetime Filtered - Double exp. fit - tau1': csv_23, 'Single Bound Lifetime Filtered - Double exp. fit - fraction1': csv_24, 'Single Bound Lifetime Filtered - Double exp. fit - tau2': csv_25, 'Single Bound Lifetime Filtered - Double exp. fit - fraction2': csv_26,'Double Bound Lifetime Filtered - Single exp. fit': csv_27, 'Double Bound Lifetime Filtered - Double exp. fit - tau1': csv_28, 'Double Bound Lifetime Filtered - Double exp. fit - fraction1': csv_29, 'Double Bound Lifetime Filtered - Double exp. fit - tau2': csv_30, 'Double Bound Lifetime Filtered - Double exp. fit - fraction2': csv_31, 'Unbound Lifetime Unfiltered - Single exp. fit': csv_32, 'Unbound Lifetime Unfiltered - Double exp. fit - tau1': csv_33, 'Unbound Lifetime Unfiltered - Double exp. fit - fraction1': csv_34, 'Unbound Lifetime Unfiltered - Double exp. fit - tau2': csv_35, 'Unbound Lifetime Unfiltered - Double exp. fit - fraction2': csv_36, 'Bound Lifetime Unfiltered - Double exp. fit - tau1': csv_37, 'Bound Lifetime Unfiltered - Double exp. fit - fraction1': csv_38, 'Bound Lifetime Unfiltered - Double exp. fit - tau2': csv_39, 'Bound Lifetime Unfiltered - Double exp. fit - fraction2': csv_40, 'Single Bound Lifetime Unfiltered - Single exp. fit': csv_41, 'Single Bound Lifetime Unfiltered - Double exp. fit - tau1': csv_42, 'Single Bound Lifetime Unfiltered - Double exp. fit - fraction1': csv_43, 'Single Bound Lifetime Unfiltered - Double exp. fit - tau2': csv_44, 'Single Bound Lifetime Unfiltered - Double exp. fit - fraction2': csv_45,'Double Bound Lifetime Unfiltered - Single exp. fit': csv_46, 'Double Bound Lifetime Unfiltered - Double exp. fit - tau1': csv_47, 'Double Bound Lifetime Unfiltered - Double exp. fit - fraction1': csv_48, 'Double Bound Lifetime Unfiltered - Double exp. fit - tau2': csv_49, 'Double Bound Lifetime Unfiltered - Double exp. fit - fraction2': csv_50}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('output_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
